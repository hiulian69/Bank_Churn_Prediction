{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RarNoE4PSXyr"
   },
   "source": [
    "# Bank data analysis\n",
    "\n",
    "The goal of your project is to create a robust classifier and use the data, where you will build a model that will recognize whether specific client will leave/unsubscribe the bank services.\n",
    "Make feature engineering but also try differnet models in order to get as much accuracy as possible.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfCA9MCqxs55"
   },
   "source": [
    "## Dataset Info\n",
    "\n",
    "* CLIENTNUM\n",
    "  - Client number. Unique identifier for the customer holding the account\n",
    "\n",
    "* Attrition_Flag (This is your target variable!)\n",
    "  - Internal event (customer activity) variable - if the account is closed then 1 else 0\n",
    "\n",
    "* Customer_Age\n",
    "  - Demographic variable - Customer's Age in Years\n",
    "\n",
    "* Gender\n",
    "  - Demographic variable - M=Male, F=Female\n",
    "\n",
    "* Dependent_count\n",
    "  - Demographic variable - Number of dependents\n",
    "\n",
    "* Education_Level\n",
    "  - Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)\n",
    "\n",
    "\n",
    "* Marital_Status\n",
    "  - Demographic variable - Married, Single, Divorced, Unknown\n",
    "\n",
    "* Income_Category\n",
    "  - Demographic variable - Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, > $120K, Unknown)\n",
    "\n",
    "\n",
    "* Card_Category\n",
    "  - Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n",
    "\n",
    "* Months_on_book\n",
    "  - Period of relationship with bank\n",
    "\n",
    "\n",
    "* Total_Relationship_Count\n",
    "  - Total no. of products held by the customer\n",
    "\n",
    "* Months_Inactive_12_mon\n",
    "  - No. of months inactive in the last 12 months\n",
    "\n",
    "* Contacts_Count_12_mon\n",
    "  - No. of Contacts in the last 12 months\n",
    "\n",
    "* Credit_Limit\n",
    "  - Credit Limit on the Credit Card\n",
    "\n",
    "* Total_Revolving_Bal\n",
    "  - Total Revolving Balance on the Credit Card\n",
    "\n",
    "* Avg_Open_To_Buy\n",
    "  - Open to Buy Credit Line (Average of last 12 months)\n",
    "\n",
    "* Total_Amt_Chng_Q4_Q1\n",
    "  - Change in Transaction Amount (Q4 over Q1)\n",
    "\n",
    "* Total_Trans_Amt\n",
    "  - Total Transaction Amount (Last 12 months)\n",
    "\n",
    "* Total_Trans_Ct\n",
    "  - Total Transaction Count (Last 12 months)\n",
    "\n",
    "* Total_Ct_Chng_Q4_Q1\n",
    "  - Change in Transaction Count (Q4 over Q1)\n",
    "\n",
    "* Avg_Utilization_Ratio\n",
    "  - Average Card Utilization Ratio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u823cXPI0QNc"
   },
   "source": [
    "# Working Plan\n",
    "\n",
    "\n",
    "\n",
    "1. Phase 1 : Dataset\n",
    "    * Team Planning\n",
    "    * Full git project Integration\n",
    "    * General Project Research\n",
    "    * Dataset Preparation\n",
    "    * Dataset Feature Engineering\n",
    "\n",
    "2. Phase 2 : Training\n",
    "    * Make Research about your model\n",
    "    * Compose your model (try different models) \n",
    "    * Ping Pong phase with Dataset feature engineers\n",
    "    * Generate more data if needed\n",
    "    * Fine tunning of your model\n",
    "\n",
    "3. Phase 3 : Deployment\n",
    "    * Perform benchmark (precision/recall), ROC curve\n",
    "    * Model Deploy (Git)\n",
    "    * Write git Readme.md file\n",
    "    * Receive Feedback from PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIGo_fGNSXys"
   },
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xj5WSaLYSXys"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline,make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ChurnPrediction/churn-prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcK1sU4iSXyw"
   },
   "source": [
    "## 1. Data preprocessing, normalization, missing data, categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgOelHW2SXyx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def one_hot_encode(df, column, prefix):# found this function which is b\n",
    "    df = df.copy()\n",
    "    dummies = pd.get_dummies(df[column], prefix=prefix)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(df, scale=False, one_hot=False , age_groupe = False ):\n",
    "    \n",
    "    #drop ID - has only unique values\n",
    "    df = df.drop('CLIENTNUM', axis=1)\n",
    "    \n",
    "    # if True create  Age_groupe\n",
    "    if age_groupe == True:\n",
    "        list_AgeGroup = [df]\n",
    "        for column in list_AgeGroup:\n",
    "            df.loc[column[\"Customer_Age\"] < 18,  'age_group'] = 18\n",
    "            df.loc[(column[\"Customer_Age\"] >= 19) & (column[\"Customer_Age\"] <= 29), 'age_group'] = 20\n",
    "            df.loc[(column[\"Customer_Age\"] >= 30) & (column[\"Customer_Age\"] <= 39), 'age_group'] = 30\n",
    "            df.loc[(column[\"Customer_Age\"] >= 40) & (column[\"Customer_Age\"] <= 49), 'age_group'] = 40\n",
    "            df.loc[(column[\"Customer_Age\"] >= 50) & (column[\"Customer_Age\"] <= 59), 'age_group'] = 50\n",
    "            df.loc[column[\"Customer_Age\"] >= 60, 'age_group'] = 60\n",
    "        \n",
    "    \n",
    "    # handle unknown values\n",
    "    df['Income_Category'] = df['Income_Category'].replace('Unknown', np.NaN)\n",
    "   \n",
    "    \n",
    "    # Fill ordinal missing values with modes ( Income_Category column)\n",
    "    df['Income_Category'] = df['Income_Category'].fillna('Less than $40K')\n",
    "    \n",
    "    # handle unknown values of marital status\n",
    "    df['Marital_Status'] = df['Marital_Status'].replace('Unknown', np.NaN)\n",
    "    \n",
    "    # Fill missing values with dominant value ( Marital_Status column)\n",
    "    df['Marital_Status'] = df['Marital_Status'].fillna(df['Marital_Status'].value_counts().index[0])\n",
    "\n",
    "    #Ordinal Variables Encoding\n",
    "\n",
    "    Income_Category_map = {\n",
    "    'Less than $40K' : 0,\n",
    "    '$40K - $60K'    : 1,\n",
    "    '$60K - $80K'    : 2,\n",
    "    '$80K - $120K'   : 3,\n",
    "    '$120K +'        : 4\n",
    "    \n",
    "    }\n",
    "    Card_Category_map = {\n",
    "    'Blue'     : 0,\n",
    "    'Silver'   : 1,\n",
    "    'Gold'     : 2,\n",
    "    'Platinum' : 3\n",
    "    }\n",
    "\n",
    "\n",
    "    Attrition_Flag_map = {\n",
    "    'Existing Customer' : 0,\n",
    "    'Attrited Customer' : 1\n",
    "    }\n",
    "\n",
    "    # Too many Unknown value to exclude it\n",
    "    Education_Level_map = {\n",
    "    'Uneducated'    : 0,\n",
    "    'High School'   : 1,\n",
    "    'College'       : 2,\n",
    "    'Graduate'      : 3,\n",
    "    'Post-Graduate' : 4,\n",
    "    'Doctorate'     : 5,\n",
    "    'Unknown'       : 6\n",
    "    }\n",
    "    \n",
    "    Gender_Map = {\n",
    "        'M' : 0,\n",
    "        'F' : 1\n",
    "    }\n",
    "\n",
    "    df.loc[:, 'Income_Category'] = df['Income_Category'].map(Income_Category_map)\n",
    "    df.loc[:, 'Attrition_Flag'] = df['Attrition_Flag'].map(Attrition_Flag_map)\n",
    "    df.loc[:, 'Education_Level'] = df['Education_Level'].map(Education_Level_map)\n",
    "    df.loc[:, 'Gender'] = df['Gender'].map(Gender_Map)\n",
    "    \n",
    "    #encoding using the function above creating + deleting old columns\n",
    "    if one_hot == True:\n",
    "        \n",
    "        df = one_hot_encode(df, 'Marital_Status', prefix='MS')\n",
    "        df = one_hot_encode(df, 'Card_Category', prefix='CC')\n",
    "    else:\n",
    "        df.loc[:, 'Card_Category'] = df['Card_Category'].map(Card_Category_map)\n",
    "    \n",
    "    # Label Encoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # the remaining categorical data are 'objects' as datatyes\n",
    "    categ = [x for x in df.columns if df[x].dtype == 'object']\n",
    "    \n",
    "    #fit_transform on each categorical column\n",
    "    for a in categ:\n",
    "        df.loc[:, a]=le.fit_transform(df.loc[:,a])\n",
    "        \n",
    "   \n",
    "    #X[\"Total_Trans_Ct_Categorical\"] = pd.cut(X.Total_Trans_Ct,bins = 16, labels = range(1,17)) \n",
    "    #X[\"Total_Trans_Amt_Categorical\"] =pd.cut(X.Total_Trans_Amt,bins = 16, labels = range(1,17))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    y = df.Attrition_Flag \n",
    "    X = df.drop('Attrition_Flag', axis=1)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    #Scale X\n",
    "    if scale == True:\n",
    "        scaler = StandardScaler()\n",
    "        ro_sc = RobustScaler()\n",
    "\n",
    "        X_ =X.loc[:,['Customer_Age', \n",
    "                     'Dependent_count',\n",
    "                     'Months_on_book', \n",
    "                     'Total_Relationship_Count',\n",
    "                     'Months_Inactive_12_mon',\n",
    "                     'Contacts_Count_12_mon',\n",
    "                     'Credit_Limit',\n",
    "                     'Total_Revolving_Bal',\n",
    "                     'Avg_Open_To_Buy',\n",
    "                     'Total_Amt_Chng_Q4_Q1',\n",
    "                     'Total_Trans_Amt',\n",
    "                     'Total_Trans_Ct', \n",
    "                     'Total_Ct_Chng_Q4_Q1',\n",
    "                     'Avg_Utilization_Ratio']]\n",
    "        \n",
    "        X = X.drop(['Customer_Age',\n",
    "                    'Dependent_count',\n",
    "                    'Months_on_book',\n",
    "                    'Total_Relationship_Count',\n",
    "                    'Months_Inactive_12_mon',\n",
    "                    'Contacts_Count_12_mon',\n",
    "                    'Credit_Limit',\n",
    "                    'Total_Revolving_Bal',\n",
    "                    'Avg_Open_To_Buy', \n",
    "                    'Total_Amt_Chng_Q4_Q1',\n",
    "                    'Total_Trans_Amt',\n",
    "                    'Total_Trans_Ct', \n",
    "                    'Total_Ct_Chng_Q4_Q1',\n",
    "                    'Avg_Utilization_Ratio'], axis= 1)\n",
    "    \n",
    "        X_ = pd.DataFrame(scaler.fit_transform(X_), columns=X_.columns)\n",
    "        \n",
    "        X = X.merge(X_, left_index = True, right_index = True)\n",
    "        \n",
    "        \n",
    "        X_2 = X.loc[:,['Credit_Limit', 'Total_Amt_Chng_Q4_Q1','Total_Trans_Amt' ]]\n",
    "        X = X.drop(['Credit_Limit',  'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt'], axis = 1)\n",
    "        \n",
    "        X_2 = pd.DataFrame(ro_sc.fit_transform(X_2), columns= X_2.columns)\n",
    "        X = X.merge(X_2, left_index = True, right_index = True)\n",
    "       \n",
    "    \n",
    "        #create bins for bimodal continious data\n",
    "        X[\"Total_Trans_Ct_Categorical\"] = pd.cut(X.Total_Trans_Ct,bins = 16, labels = range(1,17)) \n",
    "        X[\"Total_Trans_Amt_Categorical\"] =pd.cut(X.Total_Trans_Amt,bins = 16, labels = range(1,17))\n",
    "\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoding with CatBoost Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_boost(df):\n",
    "    \n",
    "    \n",
    "    # handle unknown values\n",
    "    df['Income_Category'] = df['Income_Category'].replace('Unknown', np.NaN)\n",
    "   \n",
    "    \n",
    "    # Fill ordinal missing values with modes ( Income_Category column)\n",
    "    df['Income_Category'] = df['Income_Category'].fillna('Less than $40K')\n",
    "    \n",
    "    # handle unknown values of marital status\n",
    "    df['Marital_Status'] = df['Marital_Status'].replace('Unknown', np.NaN)\n",
    "    \n",
    "    # Fill missing values with dominant value ( Marital_Status column)\n",
    "    df['Marital_Status'] = df['Marital_Status'].fillna(df['Marital_Status'].value_counts().index[0])\n",
    "    \n",
    "    # drop ID - has only unique values\n",
    "    df = df.drop('CLIENTNUM', axis=1)\n",
    "    \n",
    "    Attrition_Flag_map = {\n",
    "    'Existing Customer' : 0,\n",
    "    'Attrited Customer' : 1\n",
    "    }\n",
    "    \n",
    "    df.loc[:, 'Attrition_Flag'] = df['Attrition_Flag'].map(Attrition_Flag_map)   \n",
    "        \n",
    "    y = df.Attrition_Flag \n",
    "    X = df.drop('Attrition_Flag', axis=1)\n",
    "    \n",
    "    columns = list(X.columns)\n",
    "   \n",
    "    # the remaining categorical data are 'objects' as datatyes\n",
    "    categ = [x for x in df.columns if df[x].dtype == 'object']\n",
    "    categ.append('Dependent_count')\n",
    "    categ.append('Months_on_book')\n",
    "    categ.append('Total_Relationship_Count')\n",
    "    categ.append('Credit_Limit')\n",
    "\n",
    "    numerical = list(set(columns) - set(categ))\n",
    "    # Define catboost encoder\n",
    "    cbe_encoder = CatBoostEncoder()  \n",
    "    \n",
    "    df_cat = X.loc[:, categ]\n",
    "    df_num = X.loc[:, numerical]\n",
    "    X = X.drop(numerical, axis = 1)\n",
    "\n",
    "    # Fit encoder and transform the features\n",
    "    cbe_encoder.fit(X, y)\n",
    "    X = cbe_encoder.transform(X)\n",
    "    \n",
    "    X = X.merge(df_num, left_index = True, right_index = True)\n",
    "\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiazHmxRSXy0"
   },
   "source": [
    "## 2. Feature Anaysis, Extraction & Selection\n",
    "(you may need to perform feature selection after creating default models and compare to them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1QIHDpbSXy0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92THQSXGSXy3"
   },
   "source": [
    "## 3. Classification models\n",
    "- classical classification models\n",
    "- deep neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve5_cjilSXy4"
   },
   "source": [
    "### Random Forrest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.946 (0.007)\n"
     ]
    }
   ],
   "source": [
    "#Process data for Tree Based Alg.\n",
    "X, y = preprocess_data(data, scale=False, one_hot=False )\n",
    "\n",
    "rand_for_clf = RandomForestClassifier(max_depth=8, random_state=1)\n",
    "rand_for_clf.fit(X, y)\n",
    "\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "n_scores = cross_val_score(rand_for_clf, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "models.append((\"Random Forest\", rand_for_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.905 (0.008)\n"
     ]
    }
   ],
   "source": [
    "# We reprocess data in favour for the classification algorithm\n",
    "X, y = preprocess_data(data , scale =True, one_hot= True)\n",
    "\n",
    "LogReg_clf = LogisticRegression(random_state = 1)\n",
    "\n",
    "\n",
    "n_scores = cross_val_score(LogReg_clf, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "models.append((\"Logisitc Regression\", LogReg_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961 (0.005)\n"
     ]
    }
   ],
   "source": [
    "# We reprocess data in favour for the classification algorithm\n",
    "X, y = preprocess_data(data , scale =True, one_hot= True)\n",
    "\n",
    "ada_boost = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "\n",
    "n_scores = cross_val_score(ada_boost, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "models.append((\"AdaBoost\", ada_boost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961 (0.005)\n"
     ]
    }
   ],
   "source": [
    "X, y = preprocess_data(data, scale=False, one_hot=False)\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=150, learning_rate = 0.1, max_features=5, max_depth = 2, random_state = 0)\n",
    "\n",
    "n_scores = cross_val_score(ada_boost, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "models.append((\"Gradient Boosting\", gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961 (0.005)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X, y = preprocess_data(data, scale=False, one_hot=False)\n",
    "\n",
    "\n",
    "svc = SVC(kernel = 'linear', random_state = 0)\n",
    "\n",
    "n_scores = cross_val_score(ada_boost, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "models.append((\"SVM Classifier\", svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighbors  Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.891 (0.007)\n"
     ]
    }
   ],
   "source": [
    "X, y = preprocess_data(data, scale=False, one_hot=False)\n",
    "\n",
    "k_neig = KNeighborsClassifier(n_neighbors = 5, metric =\"minkowski\", p = 2)\n",
    "\n",
    "n_scores = cross_val_score(k_neig, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "models.append((\"KNeighbors Classifier\", k_neig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956 (0.006)\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "X, y = cat_boost(data)\n",
    "\n",
    "cat = CatBoostClassifier(n_estimators=150, learning_rate = 0.1, max_depth = 2, random_state = 0)\n",
    "\n",
    "n_scores = cross_val_score(cat , X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n",
    "models.append((\"CatBoost Classifier\", cat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc_results = []\n",
    "auc_results = []\n",
    "names = []\n",
    "\n",
    "#col names for metric table\n",
    "\n",
    "col = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', 'Accuracy Mean', 'Accuracy STD']\n",
    "\n",
    "model_results = pd.DataFrame(columns=col)\n",
    "i = 0\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "  kfold = RepeatedStratifiedKFold(n_splits = 10, random_state = 1 )\n",
    "\n",
    "  cv_acc_results = cross_val_score(model, X, y, cv=kfold, scoring = \"accuracy\")\n",
    "\n",
    "  #cv_auc_results = cross_val_score(model, X, y, cv=kfold, scoring = \"roc_auc\" )\n",
    "\n",
    "  acc_results.append(cv_acc_results)\n",
    "  #auc_results.append(cv_auc_results)\n",
    "  names.append(name)\n",
    "  model_results.loc[i] = [name, \n",
    "                         #round(cv_auc_results.mean()*100, 2),\n",
    "                         #round(cv_auc_results.std()*100, 2),\n",
    "                         round(cv_acc_results.mean()*100, 2),\n",
    "                         round(cv_acc_results.std()*100, 2)]\n",
    "  i += 1\n",
    "\n",
    "model_results = model_results.sort_values(by = [\"ROC AUC Mean\"], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGyhoU96SXy7"
   },
   "source": [
    "## 4. Evaluation and comparisons, various metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0_vsmimSXy8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4TYzMqQSXy_"
   },
   "source": [
    "## 5. Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5DpSTA-SXy_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1Uyu5APSXzD"
   },
   "source": [
    "## 6. Final evaluations and comparisons\n",
    "- the best model - analyze it in details, evaluate it with different train/test splits. Is it robust enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJYe6t-oSXzD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yo2Rs_5SXzG"
   },
   "source": [
    "## 7. Discussion, Concusions, Future improvements\n",
    "- which features are the most important\n",
    "- how will you explain the model to the management of the bank\n",
    "- how much benefit/improvement should the bank expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSU-oBptSXzG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bank_template_Churn_Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
